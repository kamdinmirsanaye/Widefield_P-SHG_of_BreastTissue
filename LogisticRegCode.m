function [trainedClassifier,ConfuseMat] = LogisticRegCode(trainingData)

% This function was first automatically generated by the classifier
% training app. It was modified by Kamdin Mirsanaye in 2021 to allow for;
% 1) Classification of datasets with different number columns
% 2) Compute the confusion matrix with 15 elements (see the last line),
% including threshold-independent parameters such as AUROC and Brier score

%   Input:
%       trainingData: the training data with numerical response as the first column.
%
%   Output:
%       trainedClassifier: a struct containing the trained classifier.
%        The struct contains various fields with information about the
%        trained classifier.
%
%       trainedClassifier.predictFcn: a function to make predictions
%        on new data. It takes an input of the same form as this training
%        code (table or matrix) and returns predictions for the response.
%        If you supply a matrix, include only the predictors columns (or
%        rows).
%
%       ConfuseMat: an array containing the performance metrics
%
%
%  For example, to train a classifier with dataset T, enter:
%  
%    [trainedClassifier,ConfuseMat] = LogisticRegCode(T)
%
%  To make predictions with the returned 'trainedClassifier' on new data Y
%  exclude the response column and enter
%
%    yfit = trainedClassifier.predictFcn(Y)


% Extract predictors and response
% This code processes the data into the right shape for training the
% classifier.
NumberOfColumns = size(trainingData,2);
ColumnNames = arrayfun(@char, sym('column_%d',[1 NumberOfColumns]),'uniform', 0);

% Convert input to table
% inputTable = array2table(trainingData, 'VariableNames', {'column_1', 'column_2', 'column_3', 'column_4', 'column_5', 'column_6', 'column_7', 'column_8', 'column_9', 'column_10', 'column_11', 'column_12', 'column_13', 'column_14', 'column_15', 'column_16', 'column_17', 'column_18', 'column_19', 'column_20', 'column_21', 'column_22', 'column_23', 'column_24', 'column_25', 'column_26', 'column_27', 'column_28', 'column_29', 'column_30', 'column_31', 'column_32', 'column_33', 'column_34'});
% predictorNames = {'column_2', 'column_3', 'column_4', 'column_5', 'column_6', 'column_7', 'column_8', 'column_9', 'column_10', 'column_11', 'column_12', 'column_13', 'column_14', 'column_15', 'column_16', 'column_17', 'column_18', 'column_19', 'column_20', 'column_21', 'column_22', 'column_23', 'column_24', 'column_25', 'column_26', 'column_27', 'column_28', 'column_29', 'column_30', 'column_31', 'column_32', 'column_33', 'column_34'};
% predictors = inputTable(:, predictorNames);
% response = inputTable.column_1;
% isCategoricalPredictor = [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false];


inputTable = array2table(trainingData, 'VariableNames', ColumnNames);
predictorNames = ColumnNames(2:end);
predictors = inputTable(:, predictorNames);
response = inputTable.column_1;
isCategoricalPredictor = false(1,NumberOfColumns-1);



% Train a classifier
% This code specifies all the classifier options and trains the classifier.
% For logistic regression, the response values must be converted to zeros
% and ones because the responses are assumed to follow a binomial
% distribution.
% 1 or true = 'successful' class
% 0 or false = 'failure' class
% NaN - missing response.
successClass = double(2);
failureClass = double(1);
missingClass = double(NaN);
successFailureAndMissingClasses = [successClass; failureClass; missingClass];
isMissing = isnan(response);
zeroOneResponse = double(ismember(response, successClass));
zeroOneResponse(isMissing) = NaN;
% Prepare input arguments to fitglm.
categoricalPredictorIndex = find(isCategoricalPredictor);
concatenatedPredictorsAndResponse = [predictors, table(zeroOneResponse)];

% Train using zero-one responses, specifying which predictors are
% categorical.
GeneralizedLinearModel = fitglm(...
    concatenatedPredictorsAndResponse, ...
    'Distribution', 'binomial', ...
    'link', 'logit', ...
    'CategoricalVars', categoricalPredictorIndex);

% Convert predicted probabilities to predicted class labels and scores.
convertSuccessProbsToPredictions = @(p) successFailureAndMissingClasses( ~isnan(p).*( (p<0.5) + 1 ) + isnan(p)*3 );
returnMultipleValuesFcn = @(varargin) varargin{1:max(1,nargout)};
scoresFcn = @(p) [1-p, p];
predictionsAndScoresFcn = @(p) returnMultipleValuesFcn( convertSuccessProbsToPredictions(p), scoresFcn(p) );

% Create the result struct with predict function
predictorExtractionFcn = @(x) array2table(x, 'VariableNames', predictorNames);
logisticRegressionPredictFcn = @(x) predictionsAndScoresFcn( predict(GeneralizedLinearModel, x) );
trainedClassifier.predictFcn = @(x) logisticRegressionPredictFcn(predictorExtractionFcn(x));

% Add additional fields to the result struct
trainedClassifier.GeneralizedLinearModel = GeneralizedLinearModel;
trainedClassifier.SuccessClass = successClass;
trainedClassifier.FailureClass = failureClass;
trainedClassifier.MissingClass = missingClass;
trainedClassifier.ClassNames = {successClass; failureClass};
trainedClassifier.About = 'This struct is a trained classifier exported from Classification Learner R2016a.';
trainedClassifier.HowToPredict = sprintf('To make predictions on a new predictor column matrix, X, use: \n  yfit = c.predictFcn(X) \nreplacing ''c'' with the name of the variable that is this struct, e.g. ''trainedClassifier''. \n \nX must contain exactly 36 columns because this classifier was trained using 36 predictors. \nX must contain only predictor columns in exactly the same order and format as your training \ndata. Do not include the response column or any columns you did not import into \nClassification Learner. \n \nFor more information, see <a href="matlab:helpview(fullfile(docroot, ''stats'', ''stats.map''), ''appclassification_exportmodeltoworkspace'')">How to predict using an exported model</a>.');

% Extract predictors and response
% This code processes the data into the right shape for training the
% classifier.
% Convert input to table

% inputTable = array2table(trainingData, 'VariableNames', {'column_1', 'column_2', 'column_3', 'column_4', 'column_5', 'column_6', 'column_7', 'column_8', 'column_9', 'column_10', 'column_11', 'column_12', 'column_13', 'column_14', 'column_15', 'column_16', 'column_17', 'column_18', 'column_19', 'column_20', 'column_21', 'column_22', 'column_23', 'column_24', 'column_25', 'column_26', 'column_27', 'column_28', 'column_29', 'column_30', 'column_31', 'column_32', 'column_33', 'column_34'});
% predictorNames = {'column_2', 'column_3', 'column_4', 'column_5', 'column_6', 'column_7', 'column_8', 'column_9', 'column_10', 'column_11', 'column_12', 'column_13', 'column_14', 'column_15', 'column_16', 'column_17', 'column_18', 'column_19', 'column_20', 'column_21', 'column_22', 'column_23', 'column_24', 'column_25', 'column_26', 'column_27', 'column_28', 'column_29', 'column_30', 'column_31', 'column_32', 'column_33', 'column_34'};
% predictors = inputTable(:, predictorNames);
% predictors1 = inputTable(:, predictorNames);
% response = inputTable.column_1;
% isCategoricalPredictor = [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false];

inputTable = array2table(trainingData, 'VariableNames', ColumnNames);
predictorNames = ColumnNames(2:end);
predictors = inputTable(:, predictorNames);
response = inputTable.column_1;
predictors1 = inputTable(:, predictorNames);
isCategoricalPredictor = false(1,NumberOfColumns-1);

% Perform cross-validation
KFolds = 5;
cvp = cvpartition(response, 'KFold', KFolds);
% Initialize the predictions and scores to the proper sizes
validationPredictions = response;
validationPredictions1 = response;
numObservations = size(predictors, 1);
numObservations1 = size(predictors1, 1);
numClasses = 2;
validationScores = NaN(numObservations, numClasses);
validationScores1 = NaN(numObservations1, numClasses);
for fold = 1:KFolds
    trainingPredictors = predictors(cvp.training(fold), :);
    trainingPredictors1 = predictors1(cvp.training(fold), :);
    trainingResponse = response(cvp.training(fold), :);
    foldIsCategoricalPredictor = isCategoricalPredictor;
    
    % Train a classifier
    % This code specifies all the classifier options and trains the classifier.
    % For logistic regression, the response values must be converted to zeros
    % and ones because the responses are assumed to follow a binomial
    % distribution.
    % 1 or true = 'successful' class
    % 0 or false = 'failure' class
    % NaN - missing response.
    successClass = double(2);
    failureClass = double(1);
    missingClass = double(NaN);
    successFailureAndMissingClasses = [successClass; failureClass; missingClass];
    isMissing = isnan(trainingResponse);
    zeroOneResponse = double(ismember(trainingResponse, successClass));
    zeroOneResponse(isMissing) = NaN;
    % Prepare input arguments to fitglm.
    categoricalPredictorIndex = find(foldIsCategoricalPredictor);
    concatenatedPredictorsAndResponse = [trainingPredictors, table(zeroOneResponse)];
    
    % Train using zero-one responses, specifying which predictors are
    % categorical.
    GeneralizedLinearModel = fitglm(...
        concatenatedPredictorsAndResponse, ...
        'Distribution', 'binomial', ...
        'link', 'logit', ...
        'CategoricalVars', categoricalPredictorIndex);
    
    % Convert predicted probabilities to predicted class labels and scores.
    convertSuccessProbsToPredictions = @(p) successFailureAndMissingClasses( ~isnan(p).*( (p<0.5) + 1 ) + isnan(p)*3 );
    returnMultipleValuesFcn = @(varargin) varargin{1:max(1,nargout)};
    scoresFcn = @(p) [1-p, p];
    predictionsAndScoresFcn = @(p) returnMultipleValuesFcn( convertSuccessProbsToPredictions(p), scoresFcn(p) );
    
    % Create the result struct with predict function
    IntermediateFcn = @(x) predict(GeneralizedLinearModel, x);
    logisticRegressionPredictFcn = @(x) predictionsAndScoresFcn( IntermediateFcn(x) );
    validationPredictFcn = @(x) logisticRegressionPredictFcn(x);
    
    % Computing the posterior probabilities
%     testFunctionForPosteriors1 = @(x) predict(GeneralizedLinearModel, x);
    testFunctionForPosteriors2 = @(x) IntermediateFcn(x);
    
    % Add additional fields to the result struct
    aparam = cvp.test(fold);
    % Compute validation predictions and scores
    validationPredictors = predictors(aparam, :);
    validationPredictors1 = predictors1(aparam, :);
    
    [foldPredictions, foldScores] = validationPredictFcn(validationPredictors);
    
    foldPredictions1 = testFunctionForPosteriors2(validationPredictors1);
    
    % Store predictions and scores in the original order
    validationPredictions(aparam, :) = foldPredictions;
    validationScores(aparam, :) = foldScores;
    
    validationPredictions1(aparam, :) = foldPredictions1;
%     validationScores1(aparam, :) = foldScores1;
    
%     size(validationPredictions1)
%     size(validationPredictions)
end

% correctPredictions = (validationPredictions == response);
% incorrectPredictions = (validationPredictions ~= response);
% validationAccuracy = sum(correctPredictions)/length(correctPredictions);

PosteriorProbabilities = validationPredictions1;

PostProbMatrix = PosteriorProbabilities;

PostProbMatrix(response==2)=PostProbMatrix(response==2)-1;

BrierScore = sum((PostProbMatrix).^2)/length(PostProbMatrix);

[X4,Y4,T4,AUC] = perfcurve(response,PosteriorProbabilities,2);
% plot ROC
% figure; plot(X4,Y4)

validationScores;
% sum(correctPredictions)
% length(correctPredictions) - sum(correctPredictions)
% length(correctPredictions)
% response==2
% validationPredictions==2
% validationPredictions==2 & response==2
TP = sum(validationPredictions==2 & response==2);
TN = sum(validationPredictions==1 & response==1);
FP = sum(validationPredictions==2 & response==1);
FN = sum(validationPredictions==1 & response==2);

TPR = TP/(TP+FN);
TNR = TN/(TN+FP);
FPR = FP/(FP+TN);
FNR = FN/(FN+TP);

PPV = TP/(TP+FP);
NPV = TN/(TN+FN);
FDR = FP/(FP+TP);
FOR = FN/(FN+TN);

F1score = 2*PPV*TPR/(PPV+TPR);
Accuracy = (TP+TN)/(TP+TN+FP+FN);

ConfuseMat = [TN TP FN FP TNR TPR FNR FPR NPV PPV FOR FDR Accuracy F1score BrierScore AUC];
